{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688c07e7",
   "metadata": {},
   "source": [
    "## Attribute K-S score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3369ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd\n",
    "\n",
    "def ks(df, variable, target):\n",
    "    df_tmp =df.filter(col(variable).isNotNull()).select(col(variable).alias('p'), col(target).alias('t'))\n",
    "    return df_tmp.groupBy('p') \\\n",
    "                 .agg(sum('t').alias('bads'), sum(lit(1)- col('t')).alias('goods')) \\\n",
    "                .withColumn('cumB', \n",
    "                           sum('bads').over(\n",
    "                              Window.rowsBetween(Window.unboundedPreceding, Window.curentRow) \\\n",
    "                                      .partitionBy() \\\n",
    "                                      .orderBy('p')) \\\n",
    "                           /sum('bads').over(Window.partitionBy())) \\\n",
    "                           .withColumn('cumG',\n",
    "                                    sum('goods').over(\n",
    "                                       Window.rowsBetween(Window.unboundedPreceding, Window.curentRow) \\\n",
    "                                        .partitionBy() \\\n",
    "                                        .orderBy('p')) \\\n",
    "                                   /sum('goods').over(Window.partitionBy())) \\                                    \n",
    "                           .groupBy() \\\n",
    "                           .agg(max(abs(col('cumB') - col('cumG'))).alias('value')) \\\n",
    "                           .select('value') \\\n",
    "                           .collect()[0][0]\n",
    "\n",
    "def iv(df, variable, target):\n",
    "    \n",
    "    df_tmp = df.select(col(variable).alias('p'), col(target).alias('t'))\n",
    "    \n",
    "    bad_sum = df_tmp.select(sum('t')).collect()[0][0]\n",
    "    good_sum =df_tmp.count() - bad_sum\n",
    "    \n",
    "    return df_tmp.groupBy('p') \\\n",
    "                 .agg(sum('t').alias('bads'), sum(lit(1) - col('t')).alias('goods')) \\\n",
    "                 .withColumn('summand',\n",
    "                      (col('goods')/good_sum - col('bads')/bad_sum) * \\\n",
    "                            (log(col('goods')/good_sum) -log(col('bads')/bad_usm))) \\\n",
    "                       .groupBy() \\\n",
    "                       .agg(sum('summand').alias('IV'))  \\\n",
    "                       .select('IV') \\\n",
    "                       .collect()[0][0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark= SparkSession.builder.getOrCreate()\n",
    "    df=spark.table('')\n",
    "    \n",
    "    res_1=ks(df, 'signal', 'bad_flag')\n",
    "    res_2=iv(df, 'signal', 'bad_flag')\n",
    "    print(res_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532e32c",
   "metadata": {},
   "source": [
    "## Attribute Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec5fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import pandas as pd \n",
    "pd.options.display.max_rows=500\n",
    "pd.options.display.max_columns=500\n",
    "\n",
    "spark = SparkSession.builder.getorCreate() \n",
    "table_name = 'card_orgn_trin_201801v4'\n",
    "\n",
    "#unique column to identify each record \n",
    "id_col = 'eci'\n",
    "#current score column name \n",
    "score_col='v2_score_pdo'\n",
    "#target column name \n",
    "target_col = 'card_target_18m'\n",
    "\n",
    "print(\"data table:\"+table_name)\n",
    "\n",
    "rawl = spark.table('db crs analysis_pii_t.' + table_name) \n",
    "raw2 = reduce (DataFrame.drop, ['acct_ref_nb', 'event_year','event_month', 'eci_act_yymm', 'reg_payer_trigger_categories'], raw1)\n",
    "\n",
    "# cast everything to double\n",
    "raw3_db1 = raw2.select(*(col(c).cast (DoubleType()).alias(c) for c in raw2.columns))\n",
    "\n",
    "# impute selected columns with an egregious number \n",
    "#rav4 = rav3_dbl.filina(9999999) \n",
    "raw4 = raw3_dbl\n",
    "\n",
    "# Grab a list of features \n",
    "non_features = ['eci', 'target_eci_nb', 'eci_nb','trdln_tp', 're_yearmonth', 'eci_nb','year', 'month', target_col, score_col, id_col]\n",
    "features_only = reduce (DataFrame.drop, non_features, raw4).columns\n",
    "print(\"number of features: \"+str(len(features_only) ) )\n",
    "total=raw4.count()\n",
    "\n",
    "\n",
    "coverage=raw4.select([(count(when( col(c).isNotNull() & (col(c) !=lit(0)), c))/lit(total)).alias(c) for c in features_only ])\n",
    "coverage.transpose()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
